{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Finetuning of large language models to perform text summarization\n",
        "\n",
        "Install all necessary libraries, which are not preinstalled on Google colab.\n"
      ],
      "metadata": {
        "id": "I5eT4mPqorlD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers --quiet\n",
        "!pip install torch --quiet\n",
        "!pip install loralib --quiet\n",
        "!pip install peft --quiet\n",
        "!pip install datasets --quiet\n",
        "!pip install py7zr --quiet\n",
        "!pip install evaluate rouge_score --quiet"
      ],
      "metadata": {
        "id": "DBKHUc67osyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import all necessary packages."
      ],
      "metadata": {
        "id": "tYFndXkepC4_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from transformers import AutoModelForSeq2SeqLM\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import GenerationConfig\n",
        "\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "xbJ7SEXgo0NZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the FLAN T5 pretrained model. It is a seq2seq type of model. We can experiment with varius models and model sizes: https://huggingface.co/google."
      ],
      "metadata": {
        "id": "6QjfSoqmpcEe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name='google/flan-t5-small'\n",
        "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "original_tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
      ],
      "metadata": {
        "id": "SOkrt7iHpNJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Count the number of parameters in the model."
      ],
      "metadata": {
        "id": "Ae6yB4bHqS4P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parameters = 0\n",
        "for _, param in original_model.named_parameters():\n",
        "  parameters += param.numel()\n",
        "\n",
        "print (f\"Number of model parameters: {parameters}.\")"
      ],
      "metadata": {
        "id": "ScnDH5lypfZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preprocessing\n",
        "\n",
        "We need to donwload the data first."
      ],
      "metadata": {
        "id": "uX9UW3cUqfvh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "import pandas as pd\n",
        "import os\n",
        "from datasets import Dataset, DatasetDict\n",
        "import json\n",
        "\n",
        "def load_samsum_from_kaggle():\n",
        "    \"\"\"\n",
        "    Download and load SAMSum dataset from Kaggle in HuggingFace format\n",
        "    Returns a DatasetDict similar to load_dataset(\"samsum\")\n",
        "    \"\"\"\n",
        "\n",
        "    # Download latest version\n",
        "    path = kagglehub.dataset_download(\"nileshmalode1/samsum-dataset-text-summarization\")\n",
        "    print(f\"Dataset downloaded to: {path}\")\n",
        "\n",
        "    # List files in the downloaded directory\n",
        "    files = os.listdir(path)\n",
        "    print(f\"Available files: {files}\")\n",
        "\n",
        "    # Initialize dictionary to store datasets\n",
        "    dataset_dict = {}\n",
        "\n",
        "    # Common file patterns for SAMSum dataset\n",
        "    possible_files = {\n",
        "        'train': ['train.csv', 'train.json', 'samsum_train.csv'],\n",
        "        'validation': ['val.csv', 'validation.csv', 'valid.csv', 'val.json', 'validation.json', 'samsum_val.csv'],\n",
        "        'test': ['test.csv', 'test.json', 'samsum_test.csv']\n",
        "    }\n",
        "\n",
        "    # Function to load CSV or JSON file\n",
        "    def load_file(filepath):\n",
        "        if filepath.endswith('.csv'):\n",
        "            df = pd.read_csv(filepath)\n",
        "\n",
        "            # Handle missing values - replace NaN with empty strings\n",
        "            df = df.fillna('')\n",
        "\n",
        "            # Convert all columns to string to avoid type issues\n",
        "            for col in df.columns:\n",
        "                df[col] = df[col].astype(str)\n",
        "\n",
        "            # Remove rows where key columns are empty or 'nan'\n",
        "            if 'dialogue' in df.columns and 'summary' in df.columns:\n",
        "                df = df[(df['dialogue'] != '') & (df['dialogue'] != 'nan') &\n",
        "                       (df['summary'] != '') & (df['summary'] != 'nan')]\n",
        "            elif 'text' in df.columns and 'summary' in df.columns:\n",
        "                df = df.rename(columns={'text': 'dialogue'})\n",
        "                df = df[(df['dialogue'] != '') & (df['dialogue'] != 'nan') &\n",
        "                       (df['summary'] != '') & (df['summary'] != 'nan')]\n",
        "            elif 'conversation' in df.columns and 'summary' in df.columns:\n",
        "                df = df.rename(columns={'conversation': 'dialogue'})\n",
        "                df = df[(df['dialogue'] != '') & (df['dialogue'] != 'nan') &\n",
        "                       (df['summary'] != '') & (df['summary'] != 'nan')]\n",
        "\n",
        "            # Reset index after filtering\n",
        "            df = df.reset_index(drop=True)\n",
        "\n",
        "            return df.to_dict('records')\n",
        "        elif filepath.endswith('.json') or filepath.endswith('.jsonl'):\n",
        "            data = []\n",
        "            with open(filepath, 'r', encoding='utf-8') as f:\n",
        "                if filepath.endswith('.jsonl'):\n",
        "                    for line in f:\n",
        "                        item = json.loads(line.strip())\n",
        "                        # Clean the item - remove None values and convert to strings\n",
        "                        cleaned_item = {}\n",
        "                        for k, v in item.items():\n",
        "                            if v is not None and str(v) != 'nan':\n",
        "                                cleaned_item[k] = str(v)\n",
        "                            else:\n",
        "                                cleaned_item[k] = ''\n",
        "                        data.append(cleaned_item)\n",
        "                else:\n",
        "                    raw_data = json.load(f)\n",
        "                    # Clean each item\n",
        "                    for item in raw_data:\n",
        "                        cleaned_item = {}\n",
        "                        for k, v in item.items():\n",
        "                            if v is not None and str(v) != 'nan':\n",
        "                                cleaned_item[k] = str(v)\n",
        "                            else:\n",
        "                                cleaned_item[k] = ''\n",
        "                        data.append(cleaned_item)\n",
        "\n",
        "            # Filter out items with empty key fields\n",
        "            filtered_data = []\n",
        "            for item in data:\n",
        "                dialogue_key = None\n",
        "                summary_key = None\n",
        "\n",
        "                # Find the dialogue and summary keys\n",
        "                for key in item.keys():\n",
        "                    if key.lower() in ['dialogue', 'text', 'conversation']:\n",
        "                        dialogue_key = key\n",
        "                    elif key.lower() in ['summary', 'summarization']:\n",
        "                        summary_key = key\n",
        "\n",
        "                # Only keep items with both dialogue and summary\n",
        "                if (dialogue_key and summary_key and\n",
        "                    item[dialogue_key] and item[summary_key] and\n",
        "                    item[dialogue_key] != '' and item[summary_key] != ''):\n",
        "                    filtered_data.append(item)\n",
        "\n",
        "            return filtered_data\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported file format: {filepath}\")\n",
        "\n",
        "    # Try to find and load each split\n",
        "    for split_name, possible_names in possible_files.items():\n",
        "        found = False\n",
        "        for filename in possible_names:\n",
        "            filepath = os.path.join(path, filename)\n",
        "            if os.path.exists(filepath):\n",
        "                print(f\"Loading {split_name} from {filename}\")\n",
        "                data = load_file(filepath)\n",
        "                dataset_dict[split_name] = Dataset.from_list(data)\n",
        "                found = True\n",
        "                break\n",
        "\n",
        "        if not found:\n",
        "            print(f\"Warning: {split_name} split not found\")\n",
        "\n",
        "    # If no specific splits found, try to load any CSV/JSON file and split it\n",
        "    if not dataset_dict:\n",
        "        print(\"No split files found, looking for any data file...\")\n",
        "        data_files = [f for f in files if f.endswith(('.csv', '.json', '.jsonl'))]\n",
        "\n",
        "        if data_files:\n",
        "            print(f\"Loading data from {data_files[0]}\")\n",
        "            filepath = os.path.join(path, data_files[0])\n",
        "            data = load_file(filepath)\n",
        "\n",
        "            # Create a single dataset and optionally split it\n",
        "            full_dataset = Dataset.from_list(data)\n",
        "\n",
        "            # If dataset is large enough, create train/validation/test splits\n",
        "            if len(data) > 1000:\n",
        "                train_test = full_dataset.train_test_split(test_size=0.2, seed=42)\n",
        "                test_val = train_test['test'].train_test_split(test_size=0.5, seed=42)\n",
        "\n",
        "                dataset_dict = {\n",
        "                    'train': train_test['train'],\n",
        "                    'validation': test_val['train'],\n",
        "                    'test': test_val['test']\n",
        "                }\n",
        "            else:\n",
        "                dataset_dict = {'train': full_dataset}\n",
        "\n",
        "    if not dataset_dict:\n",
        "        raise FileNotFoundError(\"No compatible data files found in the downloaded dataset\")\n",
        "\n",
        "    # Create DatasetDict (similar to HuggingFace format)\n",
        "    dataset = DatasetDict(dataset_dict)\n",
        "\n",
        "    # Print dataset info\n",
        "    print(\"\\nDataset loaded successfully!\")\n",
        "    print(f\"Available splits: {list(dataset.keys())}\")\n",
        "    for split_name, split_data in dataset.items():\n",
        "        print(f\"{split_name}: {len(split_data)} examples\")\n",
        "        if len(split_data) > 0:\n",
        "            print(f\"  Columns: {split_data.column_names}\")\n",
        "            print(f\"  Sample: {split_data[0]}\")\n",
        "\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "7lbQ3Yug2Wys"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dataset = load_dataset(\"samsum\")\n",
        "dataset = load_samsum_from_kaggle()\n",
        "\n",
        "print(\"Train dataset size: \" + str(len(dataset['train'])))\n",
        "print(\"Test dataset size: \" +  str(len(dataset['test'])))"
      ],
      "metadata": {
        "id": "T5K4VR6MqXfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's prepare the task prompts and tokenize the whole dataset."
      ],
      "metadata": {
        "id": "8pV40be4qu4u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_prompt_length = 200\n",
        "max_answer_length = 50\n",
        "\n",
        "def tokenize_function(example):\n",
        "    start_prompt = 'Summarize the following conversation.\\n\\n'\n",
        "    end_prompt = '\\n\\nSummary: '\n",
        "    prompt = [start_prompt + dialogue + end_prompt for dialogue in example[\"dialogue\"]]\n",
        "    inputs = original_tokenizer(prompt, padding=\"max_length\", max_length = max_prompt_length, truncation=True, return_tensors=\"pt\")\n",
        "    example['input_ids'] = inputs['input_ids']\n",
        "    example['input_attn_mask'] = inputs['attention_mask']\n",
        "    example['labels'] = original_tokenizer(example[\"summary\"], padding=\"max_length\", max_length = max_answer_length, truncation=True, return_tensors=\"pt\").input_ids\n",
        "\n",
        "    return example\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "tokenized_datasets = tokenized_datasets.remove_columns(['id', 'dialogue', 'summary',])"
      ],
      "metadata": {
        "id": "47Xr9P0Aqd0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (tokenized_datasets.keys())"
      ],
      "metadata": {
        "id": "SoM15vItq_Cm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test the summarization with one-shot learning"
      ],
      "metadata": {
        "id": "ng3sxfR9rUle"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_prompt_ids = torch.IntTensor([tokenized_datasets['test']['input_ids'][125]])\n",
        "example_prompt_mask = torch.IntTensor([tokenized_datasets['test']['input_attn_mask'][125]])\n",
        "example_answer_ids = tokenized_datasets['test']['labels'][125]\n",
        "\n",
        "model_outputs_ids = original_model.generate(input_ids=example_prompt_ids, attention_mask=example_prompt_mask,\n",
        "                                        generation_config=GenerationConfig(max_new_tokens=max_answer_length,\n",
        "                                                                           num_beams=1))\n",
        "\n",
        "prompt_txt = original_tokenizer.decode(example_prompt_ids[0], skip_special_tokens=True)\n",
        "human_answer_txt = original_tokenizer.decode(example_answer_ids, skip_special_tokens=True)\n",
        "machine_answer_txt = original_tokenizer.decode(model_outputs_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(f'PROMPT:\\n{prompt_txt}')\n",
        "print('---------------------------------------------')\n",
        "print(f'HUMAN ANSWER:\\n{human_answer_txt}')\n",
        "print('---------------------------------------------')\n",
        "print(f'MACHINE ANSWER:\\n{machine_answer_txt}')"
      ],
      "metadata": {
        "id": "QS13TQvdrOZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Original model evaluation using ROUGE."
      ],
      "metadata": {
        "id": "FEwUM2FptDxV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Metric\n",
        "metric = evaluate.load(\"rouge\")\n",
        "\n",
        "def evaluate_model(example, model, max_answer_length=max_answer_length):\n",
        "\n",
        "    prompt_ids = torch.IntTensor([example['input_ids']])\n",
        "    prompt_mask = torch.IntTensor(example['input_attn_mask'])\n",
        "    machine_answer_ids = model.generate(input_ids=prompt_ids, attention_mask=example_prompt_mask,\n",
        "                                        generation_config=GenerationConfig(max_new_tokens=max_answer_length,\n",
        "                                                                           num_beams=1))\n",
        "    prediction = original_tokenizer.decode(machine_answer_ids[0], skip_special_tokens=True)\n",
        "    labels = original_tokenizer.decode(example['labels'], skip_special_tokens=True)\n",
        "\n",
        "    return prediction, labels\n",
        "\n",
        "\n",
        "predictions, references = [] , []\n",
        "i = 1\n",
        "for example in tqdm(tokenized_datasets['test']):\n",
        "    prediction,labels = evaluate_model(example, original_model)\n",
        "    predictions.append(prediction)\n",
        "    references.append(labels)\n",
        "    if i == 50:\n",
        "      break\n",
        "    i += 1\n",
        "\n",
        "# compute metric\n",
        "rouge = metric.compute(predictions=predictions, references=references, use_stemmer=True)\n",
        "\n",
        "# print results\n",
        "print(f\"\\nROUGE-1: {rouge['rouge1']* 100:2f}%\")\n",
        "print(f\"ROUGE-2: {rouge['rouge2']* 100:2f}%\")"
      ],
      "metadata": {
        "id": "bmFR_Xw6rnEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Full finetuning"
      ],
      "metadata": {
        "id": "8KltgbBXttQ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "output_dir = './dialogue-summary-full-finetuning'\n",
        "\n",
        "fully_finetuned_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    learning_rate=1e-5,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=500,\n",
        "    max_steps=5000\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model = fully_finetuned_model,\n",
        "    args = training_args,\n",
        "    train_dataset = tokenized_datasets['train'],\n",
        "    eval_dataset = tokenized_datasets['validation']\n",
        ")"
      ],
      "metadata": {
        "id": "519OvS56tluW"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Don't run the following cell (full finetuning) in Google colab. You could run out of memory."
      ],
      "metadata": {
        "id": "PkhDwwACuTan"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#trainer.train()\n",
        "\n",
        "fully_finetuned_model_path='./dialogue-summary-full-finetuning-checkpoint'\n",
        "\n",
        "#trainer.model.save_pretrained(fully_finetuned_model_path)\n",
        "#original_tokenizer.save_pretrained(fully_finetuned_model_path)"
      ],
      "metadata": {
        "id": "x4URFwiluP_y"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will load a saved finetuned model instead."
      ],
      "metadata": {
        "id": "75yodSn3uidL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://mlcollege.com/data/full-finetuning.zip\n",
        "!rm -rf ./dialogue-summary-full-finetuning-checkpoint\n",
        "!unzip full-finetuning.zip\n",
        "fully_finetuned_model = AutoModelForSeq2SeqLM.from_pretrained(fully_finetuned_model_path)"
      ],
      "metadata": {
        "id": "6VuZfGNZuojk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test summarization with the fully finetuned model."
      ],
      "metadata": {
        "id": "xZOe2fu7y2dk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_outputs_ids = fully_finetuned_model.generate(input_ids=example_prompt_ids, attention_mask=example_prompt_mask,\n",
        "                                        generation_config=GenerationConfig(max_new_tokens=max_answer_length,\n",
        "                                                                           num_beams=1))\n",
        "\n",
        "prompt_txt = original_tokenizer.decode(example_prompt_ids[0], skip_special_tokens=True)\n",
        "human_answer_txt = original_tokenizer.decode(example_answer_ids, skip_special_tokens=True)\n",
        "machine_answer_txt = original_tokenizer.decode(model_outputs_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(f'PROMPT:\\n{prompt_txt}')\n",
        "print('---------------------------------------------')\n",
        "print(f'HUMAN ANSWER:\\n{human_answer_txt}')\n",
        "print('---------------------------------------------')\n",
        "print(f'MACHINE ANSWER:\\n{machine_answer_txt}')"
      ],
      "metadata": {
        "id": "FeXUd8xmxIh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation of the fully finetuned model with ROUGE"
      ],
      "metadata": {
        "id": "tUCDrA1mzYDS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fully_finetuned_model = AutoModelForSeq2SeqLM.from_pretrained(fully_finetuned_model_path)\n",
        "\n",
        "predictions, references = [] , []\n",
        "i = 1\n",
        "for example in tqdm(tokenized_datasets['test']):\n",
        "    prediction,labels = evaluate_model(example, fully_finetuned_model)\n",
        "    predictions.append(prediction)\n",
        "    references.append(labels)\n",
        "    if i == 50:\n",
        "      break\n",
        "    i += 1\n",
        "\n",
        "# compute metric\n",
        "rouge = metric.compute(predictions=predictions, references=references, use_stemmer=True, use_aggregator=True)\n",
        "\n",
        "# print results\n",
        "print(f\"\\nROUGE-1: {rouge['rouge1']* 100:2f}%\")\n",
        "print(f\"ROUGE-2: {rouge['rouge2']* 100:2f}%\")"
      ],
      "metadata": {
        "id": "NFYCwUIAzMrt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finetuning with Low-Rank Adaptation (LoRA)\n",
        "\n",
        "r: the rank of the update matrices, expressed in int. Lower rank results in smaller update matrices with fewer trainable parameters.\n",
        "\n",
        "lora_alpha: LoRA scaling factor.\n",
        "\n",
        "target_modules: The modules (for example, attention blocks) to apply the LoRA update matrices."
      ],
      "metadata": {
        "id": "k7hBb9EV0Wux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=32,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q\", \"v\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.SEQ_2_SEQ_LM\n",
        ")"
      ],
      "metadata": {
        "id": "RanFA7KNzmv3"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model = get_peft_model(original_model, lora_config)"
      ],
      "metadata": {
        "id": "MNAgQe9d0p-F"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print the number of trainable parameters."
      ],
      "metadata": {
        "id": "jbFG9-FD08ux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parameters = 0\n",
        "for _, param in peft_model.named_parameters():\n",
        "  if param.requires_grad:\n",
        "    parameters += param.numel()\n",
        "\n",
        "print (f\"Number of trainable model parameters: {parameters}.\")"
      ],
      "metadata": {
        "id": "gseqvkBM03sn",
        "outputId": "8b41f9b6-61d5-4034-90c2-083d75670a5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of trainable model parameters: 1376256.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = './dialogue-summary-peft'\n",
        "\n",
        "peft_training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    auto_find_batch_size=True,\n",
        "    learning_rate=1e-3,\n",
        "    num_train_epochs=5,\n",
        "    logging_steps=5, #500\n",
        "    max_steps=50 #5000\n",
        ")\n",
        "\n",
        "peft_trainer = Trainer(\n",
        "    model=peft_model,\n",
        "    args=peft_training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset = tokenized_datasets['validation']\n",
        ")"
      ],
      "metadata": {
        "id": "zjB0WqT1067-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model. Complete training would require more steps than 50. We have pretrained the model in 5000 steps."
      ],
      "metadata": {
        "id": "sc-1lB_e1MJM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#peft_trainer.train()\n",
        "\n",
        "peft_model_path='./dialogue-summary-peft-checkpoint'\n",
        "\n",
        "#trainer.model.save_pretrained(peft_model_path)\n",
        "#original_tokenizer.save_pretrained(peft_model_path)"
      ],
      "metadata": {
        "id": "2URWrPiv1GaZ"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's donwload the pretrained PEFT model (trained in 5000 steps)."
      ],
      "metadata": {
        "id": "YkZ8VV0i1zcg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!wget https://mlcollege.com/data/peft.zip\n",
        "!rm -rf ./dialogue-summary-peft-checkpoint\n",
        "!unzip peft.zip\n",
        "\n",
        "peft_model = AutoModelForSeq2SeqLM.from_pretrained(peft_model_path)"
      ],
      "metadata": {
        "id": "R3iwNvPh1j35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test summarization with the LoRA model."
      ],
      "metadata": {
        "id": "dRreRbNx3ZJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_outputs_ids = fully_finetuned_model.generate(input_ids=example_prompt_ids, attention_mask=example_prompt_mask,\n",
        "                                        generation_config=GenerationConfig(max_new_tokens=max_answer_length,\n",
        "                                                                           num_beams=1))\n",
        "\n",
        "prompt_txt = original_tokenizer.decode(example_prompt_ids[0], skip_special_tokens=True)\n",
        "human_answer_txt = original_tokenizer.decode(example_answer_ids, skip_special_tokens=True)\n",
        "machine_answer_txt = original_tokenizer.decode(model_outputs_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(f'PROMPT:\\n{prompt_txt}')\n",
        "print('---------------------------------------------')\n",
        "print(f'HUMAN ANSWER:\\n{human_answer_txt}')\n",
        "print('---------------------------------------------')\n",
        "print(f'MACHINE ANSWER:\\n{machine_answer_txt}')"
      ],
      "metadata": {
        "id": "hcpy3m8F2fMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation of the LoRA model with ROUGE"
      ],
      "metadata": {
        "id": "vdevSP3L31Aq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model = AutoModelForSeq2SeqLM.from_pretrained(peft_model_path)\n",
        "\n",
        "predictions, references = [] , []\n",
        "i = 1\n",
        "for example in tqdm(tokenized_datasets['test']):\n",
        "    prediction,labels = evaluate_model(example, peft_model)\n",
        "    predictions.append(prediction)\n",
        "    references.append(labels)\n",
        "    if i == 50:\n",
        "      break\n",
        "    i += 1\n",
        "\n",
        "# compute metric\n",
        "rouge = metric.compute(predictions=predictions, references=references, use_stemmer=True, use_aggregator=True)\n",
        "\n",
        "# print results\n",
        "print(f\"\\nROUGE-1: {rouge['rouge1']* 100:2f}%\")\n",
        "print(f\"ROUGE-2: {rouge['rouge2']* 100:2f}%\")"
      ],
      "metadata": {
        "id": "MZp9PZUv3qce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k_8xANGG4-mJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}