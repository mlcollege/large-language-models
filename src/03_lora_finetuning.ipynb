{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Finetuning of large language models to perform text summarization\n",
        "\n",
        "Install all necessary libraries, which are not preinstalled on Google colab.\n"
      ],
      "metadata": {
        "id": "I5eT4mPqorlD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers --quiet\n",
        "!pip install torch --quiet\n",
        "!pip install loralib --quiet\n",
        "!pip install peft --quiet\n",
        "!pip install datasets --quiet\n",
        "!pip install py7zr --quiet\n",
        "!pip install evaluate rouge_score --quiet"
      ],
      "metadata": {
        "id": "DBKHUc67osyq"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import all necessary packages."
      ],
      "metadata": {
        "id": "tYFndXkepC4_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from transformers import AutoModelForSeq2SeqLM\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import GenerationConfig\n",
        "\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "xbJ7SEXgo0NZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the FLAN T5 pretrained model. It is a seq2seq type of model. We can experiment with varius models and model sizes: https://huggingface.co/google."
      ],
      "metadata": {
        "id": "6QjfSoqmpcEe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name='google/flan-t5-small'\n",
        "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "original_tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
      ],
      "metadata": {
        "id": "SOkrt7iHpNJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Count the number of parameters in the model."
      ],
      "metadata": {
        "id": "Ae6yB4bHqS4P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parameters = 0\n",
        "for _, param in original_model.named_parameters():\n",
        "  parameters += param.numel()\n",
        "\n",
        "print (f\"Number of model parameters: {parameters}.\")"
      ],
      "metadata": {
        "id": "ScnDH5lypfZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preprocessing\n",
        "\n",
        "We need to donwload the data first."
      ],
      "metadata": {
        "id": "uX9UW3cUqfvh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"samsum\")\n",
        "\n",
        "print(\"Train dataset size: \" + str(len(dataset['train'])))\n",
        "print(\"Test dataset size: \" +  str(len(dataset['test'])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5K4VR6MqXfL",
        "outputId": "6e26ce63-dd6e-47c5-d56c-7e814c8cdf2b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset size: 14732\n",
            "Test dataset size: 819\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's prepare the task prompts and tokenize the whole dataset."
      ],
      "metadata": {
        "id": "8pV40be4qu4u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_prompt_length = 200\n",
        "max_answer_length = 50\n",
        "\n",
        "def tokenize_function(example):\n",
        "    start_prompt = 'Summarize the following conversation.\\n\\n'\n",
        "    end_prompt = '\\n\\nSummary: '\n",
        "    prompt = [start_prompt + dialogue + end_prompt for dialogue in example[\"dialogue\"]]\n",
        "    inputs = original_tokenizer(prompt, padding=\"max_length\", max_length = max_prompt_length, truncation=True, return_tensors=\"pt\")\n",
        "    example['input_ids'] = inputs['input_ids']\n",
        "    example['input_attn_mask'] = inputs['attention_mask']\n",
        "    example['labels'] = original_tokenizer(example[\"summary\"], padding=\"max_length\", max_length = max_answer_length, truncation=True, return_tensors=\"pt\").input_ids\n",
        "\n",
        "    return example\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "tokenized_datasets = tokenized_datasets.remove_columns(['id', 'dialogue', 'summary',])"
      ],
      "metadata": {
        "id": "47Xr9P0Aqd0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (tokenized_datasets.keys())"
      ],
      "metadata": {
        "id": "SoM15vItq_Cm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test the summarization with one-shot learning"
      ],
      "metadata": {
        "id": "ng3sxfR9rUle"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_prompt_ids = torch.IntTensor([tokenized_datasets['test']['input_ids'][125]])\n",
        "example_prompt_mask = torch.IntTensor([tokenized_datasets['test']['input_attn_mask'][125]])\n",
        "example_answer_ids = tokenized_datasets['test']['labels'][125]\n",
        "\n",
        "model_outputs_ids = original_model.generate(input_ids=example_prompt_ids, attention_mask=example_prompt_mask,\n",
        "                                        generation_config=GenerationConfig(max_new_tokens=max_answer_length,\n",
        "                                                                           num_beams=1))\n",
        "\n",
        "prompt_txt = original_tokenizer.decode(example_prompt_ids[0], skip_special_tokens=True)\n",
        "human_answer_txt = original_tokenizer.decode(example_answer_ids, skip_special_tokens=True)\n",
        "machine_answer_txt = original_tokenizer.decode(model_outputs_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(f'PROMPT:\\n{prompt_txt}')\n",
        "print('---------------------------------------------')\n",
        "print(f'HUMAN ANSWER:\\n{human_answer_txt}')\n",
        "print('---------------------------------------------')\n",
        "print(f'MACHINE ANSWER:\\n{machine_answer_txt}')"
      ],
      "metadata": {
        "id": "QS13TQvdrOZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Original model evaluation using ROUGE."
      ],
      "metadata": {
        "id": "FEwUM2FptDxV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Metric\n",
        "metric = evaluate.load(\"rouge\")\n",
        "\n",
        "def evaluate_model(example, model, max_answer_length=max_answer_length):\n",
        "\n",
        "    prompt_ids = torch.IntTensor([example['input_ids']])\n",
        "    prompt_mask = torch.IntTensor(example['input_attn_mask'])\n",
        "    machine_answer_ids = model.generate(input_ids=prompt_ids, attention_mask=example_prompt_mask,\n",
        "                                        generation_config=GenerationConfig(max_new_tokens=max_answer_length,\n",
        "                                                                           num_beams=1))\n",
        "    prediction = original_tokenizer.decode(machine_answer_ids[0], skip_special_tokens=True)\n",
        "    labels = original_tokenizer.decode(example['labels'], skip_special_tokens=True)\n",
        "\n",
        "    return prediction, labels\n",
        "\n",
        "\n",
        "predictions, references = [] , []\n",
        "i = 1\n",
        "for example in tqdm(tokenized_datasets['test']):\n",
        "    prediction,labels = evaluate_model(example, original_model)\n",
        "    predictions.append(prediction)\n",
        "    references.append(labels)\n",
        "    if i == 50:\n",
        "      break\n",
        "    i += 1\n",
        "\n",
        "# compute metric\n",
        "rouge = metric.compute(predictions=predictions, references=references, use_stemmer=True)\n",
        "\n",
        "# print results\n",
        "print(f\"\\nROUGE-1: {rouge['rouge1']* 100:2f}%\")\n",
        "print(f\"ROUGE-2: {rouge['rouge2']* 100:2f}%\")"
      ],
      "metadata": {
        "id": "bmFR_Xw6rnEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Full finetuning"
      ],
      "metadata": {
        "id": "8KltgbBXttQ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "output_dir = './dialogue-summary-full-finetuning'\n",
        "\n",
        "fully_finetuned_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    learning_rate=1e-5,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=500,\n",
        "    max_steps=5000\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model = fully_finetuned_model,\n",
        "    args = training_args,\n",
        "    train_dataset = tokenized_datasets['train'],\n",
        "    eval_dataset = tokenized_datasets['validation']\n",
        ")"
      ],
      "metadata": {
        "id": "519OvS56tluW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Don't run the following cell (full finetuning) in Google colab. You could run out of memory."
      ],
      "metadata": {
        "id": "PkhDwwACuTan"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#trainer.train()\n",
        "\n",
        "fully_finetuned_model_path='./dialogue-summary-full-finetuning-checkpoint'\n",
        "\n",
        "#trainer.model.save_pretrained(fully_finetuned_model_path)\n",
        "#original_tokenizer.save_pretrained(fully_finetuned_model_path)"
      ],
      "metadata": {
        "id": "x4URFwiluP_y"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will load a saved finetuned model instead."
      ],
      "metadata": {
        "id": "75yodSn3uidL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://mlcollege.com/data/full-finetuning.zip\n",
        "!rm -rf ./dialogue-summary-full-finetuning-checkpoint\n",
        "!unzip full-finetuning.zip\n",
        "fully_finetuned_model = AutoModelForSeq2SeqLM.from_pretrained(fully_finetuned_model_path)"
      ],
      "metadata": {
        "id": "6VuZfGNZuojk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test summarization with the fully finetuned model."
      ],
      "metadata": {
        "id": "xZOe2fu7y2dk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_outputs_ids = fully_finetuned_model.generate(input_ids=example_prompt_ids, attention_mask=example_prompt_mask,\n",
        "                                        generation_config=GenerationConfig(max_new_tokens=max_answer_length,\n",
        "                                                                           num_beams=1))\n",
        "\n",
        "prompt_txt = original_tokenizer.decode(example_prompt_ids[0], skip_special_tokens=True)\n",
        "human_answer_txt = original_tokenizer.decode(example_answer_ids, skip_special_tokens=True)\n",
        "machine_answer_txt = original_tokenizer.decode(model_outputs_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(f'PROMPT:\\n{prompt_txt}')\n",
        "print('---------------------------------------------')\n",
        "print(f'HUMAN ANSWER:\\n{human_answer_txt}')\n",
        "print('---------------------------------------------')\n",
        "print(f'MACHINE ANSWER:\\n{machine_answer_txt}')"
      ],
      "metadata": {
        "id": "FeXUd8xmxIh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation of the fully finetuned model with ROUGE"
      ],
      "metadata": {
        "id": "tUCDrA1mzYDS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fully_finetuned_model = AutoModelForSeq2SeqLM.from_pretrained(fully_finetuned_model_path)\n",
        "\n",
        "predictions, references = [] , []\n",
        "i = 1\n",
        "for example in tqdm(tokenized_datasets['test']):\n",
        "    prediction,labels = evaluate_model(example, fully_finetuned_model)\n",
        "    predictions.append(prediction)\n",
        "    references.append(labels)\n",
        "    if i == 50:\n",
        "      break\n",
        "    i += 1\n",
        "\n",
        "# compute metric\n",
        "rouge = metric.compute(predictions=predictions, references=references, use_stemmer=True, use_aggregator=True)\n",
        "\n",
        "# print results\n",
        "print(f\"\\nROUGE-1: {rouge['rouge1']* 100:2f}%\")\n",
        "print(f\"ROUGE-2: {rouge['rouge2']* 100:2f}%\")"
      ],
      "metadata": {
        "id": "NFYCwUIAzMrt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finetuning with Low-Rank Adaptation (LoRA)\n",
        "\n",
        "r: the rank of the update matrices, expressed in int. Lower rank results in smaller update matrices with fewer trainable parameters.\n",
        "\n",
        "lora_alpha: LoRA scaling factor.\n",
        "\n",
        "target_modules: The modules (for example, attention blocks) to apply the LoRA update matrices."
      ],
      "metadata": {
        "id": "k7hBb9EV0Wux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=32,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q\", \"v\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.SEQ_2_SEQ_LM\n",
        ")"
      ],
      "metadata": {
        "id": "RanFA7KNzmv3"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model = get_peft_model(original_model, lora_config)"
      ],
      "metadata": {
        "id": "MNAgQe9d0p-F"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print the number of trainable parameters."
      ],
      "metadata": {
        "id": "jbFG9-FD08ux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parameters = 0\n",
        "for _, param in peft_model.named_parameters():\n",
        "  if param.requires_grad:\n",
        "    parameters += param.numel()\n",
        "\n",
        "print (f\"Number of trainable model parameters: {parameters}.\")"
      ],
      "metadata": {
        "id": "gseqvkBM03sn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = './dialogue-summary-peft'\n",
        "\n",
        "peft_training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    auto_find_batch_size=True,\n",
        "    learning_rate=1e-3,\n",
        "    num_train_epochs=5,\n",
        "    logging_steps=5, #500\n",
        "    max_steps=50 #5000\n",
        ")\n",
        "\n",
        "peft_trainer = Trainer(\n",
        "    model=peft_model,\n",
        "    args=peft_training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset = tokenized_datasets['validation']\n",
        ")"
      ],
      "metadata": {
        "id": "zjB0WqT1067-"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model. Complete training would require more steps than 50. We have pretrained the model in 5000 steps."
      ],
      "metadata": {
        "id": "sc-1lB_e1MJM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "peft_trainer.train()\n",
        "\n",
        "peft_model_path='./dialogue-summary-peft-checkpoint'\n",
        "\n",
        "trainer.model.save_pretrained(peft_model_path)\n",
        "original_tokenizer.save_pretrained(peft_model_path)"
      ],
      "metadata": {
        "id": "2URWrPiv1GaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's donwload the pretrained PEFT model (trained in 5000 steps)."
      ],
      "metadata": {
        "id": "YkZ8VV0i1zcg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!wget https://mlcollege.com/data/peft.zip\n",
        "!rm -rf ./dialogue-summary-peft-checkpoint\n",
        "!unzip peft.zip\n",
        "\n",
        "peft_model = AutoModelForSeq2SeqLM.from_pretrained(peft_model_path)"
      ],
      "metadata": {
        "id": "R3iwNvPh1j35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test summarization with the LoRA model."
      ],
      "metadata": {
        "id": "dRreRbNx3ZJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_outputs_ids = fully_finetuned_model.generate(input_ids=example_prompt_ids, attention_mask=example_prompt_mask,\n",
        "                                        generation_config=GenerationConfig(max_new_tokens=max_answer_length,\n",
        "                                                                           num_beams=1))\n",
        "\n",
        "prompt_txt = original_tokenizer.decode(example_prompt_ids[0], skip_special_tokens=True)\n",
        "human_answer_txt = original_tokenizer.decode(example_answer_ids, skip_special_tokens=True)\n",
        "machine_answer_txt = original_tokenizer.decode(model_outputs_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(f'PROMPT:\\n{prompt_txt}')\n",
        "print('---------------------------------------------')\n",
        "print(f'HUMAN ANSWER:\\n{human_answer_txt}')\n",
        "print('---------------------------------------------')\n",
        "print(f'MACHINE ANSWER:\\n{machine_answer_txt}')"
      ],
      "metadata": {
        "id": "hcpy3m8F2fMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation of the LoRA model with ROUGE"
      ],
      "metadata": {
        "id": "vdevSP3L31Aq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model = AutoModelForSeq2SeqLM.from_pretrained(peft_model_path)\n",
        "\n",
        "predictions, references = [] , []\n",
        "i = 1\n",
        "for example in tqdm(tokenized_datasets['test']):\n",
        "    prediction,labels = evaluate_model(example, peft_model)\n",
        "    predictions.append(prediction)\n",
        "    references.append(labels)\n",
        "    if i == 50:\n",
        "      break\n",
        "    i += 1\n",
        "\n",
        "# compute metric\n",
        "rouge = metric.compute(predictions=predictions, references=references, use_stemmer=True, use_aggregator=True)\n",
        "\n",
        "# print results\n",
        "print(f\"\\nROUGE-1: {rouge['rouge1']* 100:2f}%\")\n",
        "print(f\"ROUGE-2: {rouge['rouge2']* 100:2f}%\")"
      ],
      "metadata": {
        "id": "MZp9PZUv3qce"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}